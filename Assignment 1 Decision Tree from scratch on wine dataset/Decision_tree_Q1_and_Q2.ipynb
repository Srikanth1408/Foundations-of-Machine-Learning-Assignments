{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "KkkNNLp_euck",
    "outputId": "0838e722-a501-4354-fc48-05d37b99a169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        0  \n",
       "1      9.5        0  \n",
       "2     10.1        0  \n",
       "3      9.9        0  \n",
       "4      9.9        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "wine_dataset=pd.read_csv(\"wine-dataset.csv\")\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic version of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C3okAkmdQak",
    "outputId": "8439c939-1dee-4df6-a46b-73b227f05b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data attributes after removing target label is ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8490\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "\n",
    "import math,operator,random,time,copy\n",
    "import csv\n",
    "\n",
    "\n",
    "class csvdata():                                                   #Created a class CSV data to deal with the objects of data \n",
    "    def __init__(self, classifier):\n",
    "        self.rows = []\n",
    "        self.attributes = []\n",
    "        self.attribute_types = []\n",
    "        self.classifier = classifier\n",
    "        self.class_col_index = None\n",
    "\n",
    "                                                                    #the tree node class which will build the tree\n",
    "class treeNode():                                                                           \n",
    "    def __init__(self, is_leaf, classification, attribute_split_index, attribute_split_value, parent, left_child, right_child, height):\n",
    "\n",
    "   \n",
    "        self.parent = parent                                                                              \n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.height = None\n",
    "        self.is_leaf = True\n",
    "\n",
    "        self.classification = None\n",
    "        self.attribute_split = None\n",
    "        self.attribute_split_index = None\n",
    "        self.attribute_split_value = None\n",
    "\n",
    "\n",
    "\n",
    "def pre_processing(dataset):                                          #Creating a function to pre process data\n",
    "    for row in dataset.rows:\n",
    "        for i in range(len(dataset.rows[0])):\n",
    "            if dataset.attributes[i] == 'True':\n",
    "                row[i] = float(row[i])\n",
    "\n",
    "\n",
    "def compute_decision_tree(dataset, parent_node, classifier):                           #Compute a decision tree function\n",
    "    node = treeNode(True, None, None, None, parent_node, None, None, 0)\n",
    "    if (parent_node == None):\n",
    "        node.height = 0\n",
    "    else:\n",
    "        node.height = node.parent.height + 1\n",
    "\n",
    "   \n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)          #count the number of rows with classification \"1\"\n",
    "\n",
    "    if (len(dataset.rows) == ones):\n",
    "        node.classification = 1\n",
    "        node.is_leaf = True\n",
    "        return node\n",
    "    elif (ones == 0):\n",
    "        node.is_leaf = True\n",
    "        node.classification = 0\n",
    "        return node\n",
    "    else:\n",
    "        node.is_leaf = False\n",
    "\n",
    "   \n",
    "    splitting_attribute = None                                                #The index of the attribute we will split on\n",
    "\n",
    "    \n",
    "    maximum_info_gain = 0                                                    #The information gain given by the best attribute\n",
    "\n",
    "    split_val = None\n",
    "    minimum_info_gain = 0.01\n",
    "\n",
    "    entropy = calculate_entropy(dataset, classifier)                                  #Calculating Entropy \n",
    "\n",
    "    \n",
    "    for attr_index in range(len(dataset.rows[0])):                                    #for each column of data\n",
    "\n",
    "        if (dataset.attributes[attr_index] != classifier):\n",
    "            local_max_gain = 0\n",
    "            local_split_val = None\n",
    "            attr_value_list = [example[attr_index] for example in dataset.rows]\n",
    "            #these are the values we can split on, now we must find the best one\n",
    "            attr_value_list = list(set(attr_value_list))              #remove duplicates from list of all attribute values\n",
    "\n",
    "            if(len(attr_value_list) > 100):\n",
    "                attr_value_list = sorted(attr_value_list)\n",
    "                total = len(attr_value_list)\n",
    "                ten_percentile = int(total/10)\n",
    "                new_list = []\n",
    "                for x in range(1, 10):\n",
    "                    new_list.append(attr_value_list[x*ten_percentile])\n",
    "                attr_value_list = new_list\n",
    "\n",
    "            for val in attr_value_list:\n",
    "                                                                        #calculate the gain if we split on this value\n",
    "                                                      #if gain is greater than local_max_gain, save this gain and this value\n",
    "                current_gain = calculate_information_gain(attr_index, dataset,val,entropy)\n",
    "\n",
    "                if (current_gain > local_max_gain):\n",
    "                    local_max_gain = current_gain\n",
    "                    local_split_val = val\n",
    "\n",
    "            if (local_max_gain > maximum_info_gain):\n",
    "                maximum_info_gain = local_max_gain\n",
    "                split_val = local_split_val\n",
    "                splitting_attribute = attr_index\n",
    "\n",
    "    if (maximum_info_gain <= minimum_info_gain or node.height > 20):\n",
    "        node.is_leaf = True\n",
    "        node.classification = classify_leaf(dataset, classifier)\n",
    "        return node\n",
    "\n",
    "    node.attribute_split_index = splitting_attribute\n",
    "    node.attribute_split = dataset.attributes[splitting_attribute]\n",
    "    node.attribute_split_value = split_val\n",
    "\n",
    "    left_dataset = csvdata(classifier)\n",
    "    right_dataset = csvdata(classifier)\n",
    "\n",
    "    left_dataset.attributes = dataset.attributes\n",
    "    right_dataset.attributes = dataset.attributes\n",
    "\n",
    "    left_dataset.attribute_types = dataset.attribute_types\n",
    "    right_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for row in dataset.rows:\n",
    "        if (splitting_attribute is not None and row[splitting_attribute] >= split_val):\n",
    "            left_dataset.rows.append(row)\n",
    "        elif (splitting_attribute is not None):\n",
    "            right_dataset.rows.append(row)\n",
    "\n",
    "    node.left_child = compute_decision_tree(left_dataset, node, classifier)\n",
    "    node.right_child = compute_decision_tree(right_dataset, node, classifier)\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def classify_leaf(dataset, classifier):                                                  #Classify dataset\n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)\n",
    "    total = len(dataset.rows)\n",
    "    zeroes = total - ones\n",
    "    if (ones >= zeroes):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def classify(example, node, class_col_index):                                           #Final evaluation of the data\n",
    "    if (node.is_leaf == True):\n",
    "        return node.classification\n",
    "    else:\n",
    "        if (example[node.attribute_split_index] >= node.attribute_split_value):\n",
    "            return classify(example, node.left_child, class_col_index)\n",
    "        else:\n",
    "            return classify(example, node.right_child, class_col_index)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_entropy(dataset, classifier):                                     #Calculate the entropy of the current dataset\n",
    "\n",
    "    \n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)          #get count of all the rows with classification 1\n",
    "\n",
    "    \n",
    "    total_rows = len(dataset.rows);                                             #get the count of all the rows in the dataset.\n",
    "    \n",
    "\n",
    "    \n",
    "    entropy = 0                                                   #Entropy formula is sum of p*log2(p) P is the probability\n",
    "\n",
    "    \n",
    "    p = ones / total_rows                                                  #probability p of classification 1 in total data\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "    \n",
    "    p = (total_rows - ones)/total_rows                                       #probability p of classification 0 in total data\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "\n",
    "                                                                                     #from the formula\n",
    "    entropy = -entropy\n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def calculate_information_gain(attr_index, dataset, val, entropy):        #Calculate the gain of a particular attribute split\n",
    "\n",
    "    classifier = dataset.attributes[attr_index]\n",
    "    attr_entropy = 0\n",
    "    total_rows = len(dataset.rows);\n",
    "    gain_upper_dataset = csvdata(classifier)\n",
    "    gain_lower_dataset = csvdata(classifier)\n",
    "    gain_upper_dataset.attributes = dataset.attributes\n",
    "    gain_lower_dataset.attributes = dataset.attributes\n",
    "    gain_upper_dataset.attribute_types = dataset.attribute_types\n",
    "    gain_lower_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for example in dataset.rows:\n",
    "        if (example[attr_index] >= val):\n",
    "            gain_upper_dataset.rows.append(example)\n",
    "        elif (example[attr_index] < val):\n",
    "            gain_lower_dataset.rows.append(example)\n",
    "\n",
    "    if (len(gain_lower_dataset.rows) == 0 or len(gain_upper_dataset.rows) == 0):\n",
    "        return -1\n",
    "\n",
    "    attr_entropy += len(gain_upper_dataset.rows) * calculate_entropy(gain_upper_dataset, classifier) / total_rows\n",
    "    attr_entropy += len(gain_lower_dataset.rows) * calculate_entropy(gain_lower_dataset, classifier) / total_rows\n",
    "\n",
    "    return entropy - attr_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_ones(instances, attributes, classifier):                           #count number of rows with classification \"1\"\n",
    "    count = 0\n",
    "    class_col_index = None\n",
    "\n",
    "    \n",
    "    for a in range(len(attributes)):                                                #find the index of classifier\n",
    "        if attributes[a] == classifier:\n",
    "            class_col_index = a\n",
    "        else:\n",
    "            class_col_index = len(attributes) - 1\n",
    "    for i in instances:\n",
    "        if i[class_col_index] == \"1\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def validate_tree(node, dataset):\n",
    "    total = len(dataset.rows)\n",
    "    correct = 0\n",
    "    for row in dataset.rows:\n",
    "        correct += validate_row(node, row)                                                 # validate example\n",
    "    return correct/total\n",
    "\n",
    "                                                                                          # Validate row \n",
    "def validate_row(node, row):\n",
    "    if (node.is_leaf == True):\n",
    "        projected = node.classification\n",
    "        actual = int(row[-1])\n",
    "        if (projected == actual):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    value = row[node.attribute_split_index]\n",
    "    if (value >= node.attribute_split_value):\n",
    "        return validate_row(node.left_child, row)\n",
    "    else:\n",
    "        return validate_row(node.right_child, row)\n",
    "\n",
    "def run():\n",
    "\n",
    "    f = open(\"wine-dataset.csv\")\n",
    "    original_file = f.read()\n",
    "    rowsplit_data = original_file.splitlines()\n",
    "\n",
    "\n",
    "    data = csvdata(\"\")\n",
    "    train_set = csvdata(\"\")\n",
    "    test_set = csvdata(\"\")\n",
    "    data.rows = [rows.split(',') for rows in rowsplit_data]\n",
    "\n",
    "    data.attributes = data.rows.pop(0)\n",
    "    print(\"Data attributes after removing target label is\",data.attributes)\n",
    "\n",
    "                                                        # true indicates numeric data. false in nominal data\n",
    "    data.attribute_types = ['true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'false']\n",
    "\n",
    "    classifier = data.attributes[-1]\n",
    "    data.classifier = classifier\n",
    "\n",
    "\n",
    "                                                                                                   # find index of classifier\n",
    "    for a in range(len(data.attributes)):\n",
    "        if data.attributes[a] == data.classifier:\n",
    "            data.class_col_index = a\n",
    "        else:\n",
    "            data.class_col_index = range(len(data.attributes))[-1]\n",
    "\n",
    "                                                                                                   # preprocessing the dataset\n",
    "    pre_processing(data)\n",
    "\n",
    "    train_set = copy.deepcopy(data)\n",
    "    test_set = copy.deepcopy(data)\n",
    "    validate_set = copy.deepcopy(data)\n",
    "    \n",
    "    train_set.rows = []\n",
    "    test_set.rows = []\n",
    "    validate_set.rows = []\n",
    "\n",
    "    K=10\n",
    "    accuracy = []\n",
    "\n",
    "\n",
    "    for k in range(1):\n",
    "        train_set.rows = [x for i, x in enumerate(data.rows) if i % K != k]\n",
    "        test_set.rows = [x for i, x in enumerate(data.rows) if i % K == k]\n",
    "\n",
    "        print (\"Number of training records: %d\" % len(train_set.rows))\n",
    "        print (\"Number of test records: %d\" % len(test_set.rows))\n",
    "        root = compute_decision_tree(train_set, None, classifier)\n",
    "\n",
    "       \n",
    "        results = []                                             #Classify the test set using the tree we just constructed\n",
    "        for instance in test_set.rows:\n",
    "            result = classify(instance, root, test_set.class_col_index)\n",
    "            results.append(str(result) == str(instance[-1]))\n",
    "\n",
    "        \n",
    "        acc = float(results.count(True))/float(len(results))                                           #Accuracy\n",
    "        print (\"accuracy: %.4f\" % acc)\n",
    "\n",
    "        accuracy.append(acc)\n",
    "        del root\n",
    "\n",
    "    accuracy = math.fsum(accuracy)\n",
    "                                                                              #Writing results to a file\n",
    "    f = open(\"result.txt\", \"w\")\n",
    "    f.write(\"Accuracy observed : %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation using k fold k=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "classifier is 11\n",
      "Doing fold  0\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8490\n",
      "Doing fold  1\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8327\n",
      "Doing fold  2\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8204\n",
      "Doing fold  3\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8265\n",
      "Doing fold  4\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8714\n",
      "Doing fold  5\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8367\n",
      "Doing fold  6\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8122\n",
      "Doing fold  7\n",
      "Number of training records: 4408\n",
      "Number of test records: 490\n",
      "accuracy: 0.8327\n",
      "Doing fold  8\n",
      "Number of training records: 4409\n",
      "Number of test records: 489\n",
      "accuracy: 0.8282\n",
      "Doing fold  9\n",
      "Number of training records: 4409\n",
      "Number of test records: 489\n",
      "accuracy: 0.8405\n",
      "Accuracy  0.835034 \n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "\n",
    "import math,operator,random,time,copy\n",
    "import csv\n",
    "\n",
    "\n",
    "class csvdata():                                #Created a class CSV data to deal with the objects of data\n",
    "    def __init__(self, classifier):\n",
    "        self.rows = []\n",
    "        self.attributes = []\n",
    "        self.attribute_types = []\n",
    "        self.classifier = classifier\n",
    "        self.class_col_index = None\n",
    "\n",
    "class treeNode():                                #the tree node class which will build the tree\n",
    "    def __init__(self, is_leaf, classification, attribute_split_index, attribute_split_value, parent, left_child, right_child, height):\n",
    "\n",
    "        # tree\n",
    "        self.parent = parent\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.height = None\n",
    "        self.is_leaf = True\n",
    "\n",
    "        self.classification = None\n",
    "        self.attribute_split = None\n",
    "        self.attribute_split_index = None\n",
    "        self.attribute_split_value = None\n",
    "\n",
    "\n",
    "\n",
    "def pre_processing(dataset):                                   #Creating a function to pre process data\n",
    "    for row in dataset.rows:\n",
    "        for i in range(len(dataset.rows[0])):\n",
    "            if dataset.attributes[i] == 'True':\n",
    "                row[i] = float(row[i])\n",
    "\n",
    "\n",
    "def compute_decision_tree(dataset, parent_node, classifier):          #Compute a decision tree function\n",
    "    node = treeNode(True, None, None, None, parent_node, None, None, 0)\n",
    "    if (parent_node == None):\n",
    "        node.height = 0\n",
    "    else:\n",
    "        node.height = node.parent.height + 1\n",
    "\n",
    "    \n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)  #count the number of rows with classification \"1\"\n",
    "\n",
    "    if (len(dataset.rows) == ones):\n",
    "        node.classification = 1\n",
    "        node.is_leaf = True\n",
    "        return node\n",
    "    elif (ones == 0):\n",
    "        node.is_leaf = True\n",
    "        node.classification = 0\n",
    "        return node\n",
    "    else:\n",
    "        node.is_leaf = False\n",
    "\n",
    "   \n",
    "    splitting_attribute = None                                         # The index of the attribute we will split on\n",
    "\n",
    "   \n",
    "    maximum_info_gain = 0                                         # The information gain given by the best attribute\n",
    "\n",
    "    split_val = None\n",
    "    minimum_info_gain = 0.01\n",
    "\n",
    "    entropy = calculate_entropy(dataset, classifier)\n",
    "\n",
    "  \n",
    "    for attr_index in range(len(dataset.rows[0])):                 #for each column of data\n",
    "\n",
    "        if (dataset.attributes[attr_index] != classifier):\n",
    "            local_max_gain = 0\n",
    "            local_split_val = None\n",
    "            attr_value_list = [example[attr_index] for example in dataset.rows] # these are the values we can split on, now we must find the best one\n",
    "            attr_value_list = list(set(attr_value_list)) # remove duplicates from list of all attribute values\n",
    "\n",
    "            if(len(attr_value_list) > 100):\n",
    "                attr_value_list = sorted(attr_value_list)\n",
    "                total = len(attr_value_list)\n",
    "                ten_percentile = int(total/10)\n",
    "                new_list = []\n",
    "                for x in range(1, 10):\n",
    "                    new_list.append(attr_value_list[x*ten_percentile])\n",
    "                attr_value_list = new_list\n",
    "\n",
    "            for val in attr_value_list:\n",
    "                                                                             # calculate the gain if we split on this value\n",
    "                                                        # if gain is greater than local_max_gain, save this gain and this value\n",
    "                current_gain = calculate_information_gain(attr_index, dataset,val,entropy)\n",
    "\n",
    "                if (current_gain > local_max_gain):\n",
    "                    local_max_gain = current_gain\n",
    "                    local_split_val = val\n",
    "\n",
    "            if (local_max_gain > maximum_info_gain):\n",
    "                maximum_info_gain = local_max_gain\n",
    "                split_val = local_split_val\n",
    "                splitting_attribute = attr_index\n",
    "\n",
    "    if (maximum_info_gain <= minimum_info_gain or node.height > 20):\n",
    "        node.is_leaf = True\n",
    "        node.classification = classify_leaf(dataset, classifier)\n",
    "        return node\n",
    "\n",
    "    node.attribute_split_index = splitting_attribute\n",
    "    node.attribute_split = dataset.attributes[splitting_attribute]\n",
    "    node.attribute_split_value = split_val\n",
    "\n",
    "    left_dataset = csvdata(classifier)\n",
    "    right_dataset = csvdata(classifier)\n",
    "\n",
    "    left_dataset.attributes = dataset.attributes\n",
    "    right_dataset.attributes = dataset.attributes\n",
    "\n",
    "    left_dataset.attribute_types = dataset.attribute_types\n",
    "    right_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for row in dataset.rows:\n",
    "        if (splitting_attribute is not None and row[splitting_attribute] >= split_val):\n",
    "            left_dataset.rows.append(row)\n",
    "        elif (splitting_attribute is not None):\n",
    "            right_dataset.rows.append(row)\n",
    "\n",
    "    node.left_child = compute_decision_tree(left_dataset, node, classifier)\n",
    "    node.right_child = compute_decision_tree(right_dataset, node, classifier)\n",
    "\n",
    "    return node\n",
    "\n",
    "                            \n",
    "def classify_leaf(dataset, classifier):                                                   # Classify dataset\n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)\n",
    "    total = len(dataset.rows)\n",
    "    zeroes = total - ones\n",
    "    if (ones >= zeroes):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def classify(example, node, class_col_index):                                          # Final evaluation of the data\n",
    "    if (node.is_leaf == True):\n",
    "        return node.classification\n",
    "    else:\n",
    "        if (example[node.attribute_split_index] >= node.attribute_split_value):\n",
    "            return classify(example, node.left_child, class_col_index)\n",
    "        else:\n",
    "            return classify(example, node.right_child, class_col_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_entropy(dataset, classifier):                             # Calculate the entropy of the current dataset\n",
    "\n",
    "    \n",
    "    ones = count_ones(dataset.rows, dataset.attributes, classifier)  #get count of all the rows with classification 1\n",
    "\n",
    " \n",
    "    total_rows = len(dataset.rows);                                     #get the count of all the rows in the dataset.\n",
    "  \n",
    "\n",
    "    \n",
    "    entropy = 0                                 #Entropy formula is sum of p*log2(p). Referred the slides. P is the probability\n",
    "\n",
    "    \n",
    "    p = ones / total_rows                        #probability p of classification 1 in total data\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "    \n",
    "    p = (total_rows - ones)/total_rows\n",
    "    if (p != 0):\n",
    "        entropy += p * math.log(p, 2)\n",
    "\n",
    "   \n",
    "    entropy = -entropy                                #from the formula\n",
    "    return entropy\n",
    "\n",
    "\n",
    "                                                               # Calculate the gain of a particular attribute split\n",
    "def calculate_information_gain(attr_index, dataset, val, entropy):\n",
    "\n",
    "    classifier = dataset.attributes[attr_index]\n",
    "    attr_entropy = 0\n",
    "    total_rows = len(dataset.rows);\n",
    "    gain_upper_dataset = csvdata(classifier)\n",
    "    gain_lower_dataset = csvdata(classifier)\n",
    "    gain_upper_dataset.attributes = dataset.attributes\n",
    "    gain_lower_dataset.attributes = dataset.attributes\n",
    "    gain_upper_dataset.attribute_types = dataset.attribute_types\n",
    "    gain_lower_dataset.attribute_types = dataset.attribute_types\n",
    "\n",
    "    for example in dataset.rows:\n",
    "        if (example[attr_index] >= val):\n",
    "            gain_upper_dataset.rows.append(example)\n",
    "        elif (example[attr_index] < val):\n",
    "            gain_lower_dataset.rows.append(example)\n",
    "\n",
    "    if (len(gain_lower_dataset.rows) == 0 or len(gain_upper_dataset.rows) == 0):\n",
    "        return -1\n",
    "\n",
    "    attr_entropy += len(gain_upper_dataset.rows) * calculate_entropy(gain_upper_dataset, classifier) / total_rows\n",
    "    attr_entropy += len(gain_lower_dataset.rows) * calculate_entropy(gain_lower_dataset, classifier) / total_rows\n",
    "\n",
    "    return entropy - attr_entropy\n",
    "\n",
    "           \n",
    "\n",
    "def count_ones(instances, attributes, classifier):      # count number of rows with classification \"1\"\n",
    "    count = 0\n",
    "    class_col_index = None\n",
    "\n",
    "    \n",
    "    for a in range(len(attributes)):                    #find the index of classifier\n",
    "        if attributes[a] == classifier:\n",
    "            class_col_index = a\n",
    "        else:\n",
    "            class_col_index = len(attributes) - 1\n",
    "    for i in instances:\n",
    "        if i[class_col_index] == \"1\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def validate_tree(node, dataset):\n",
    "    total = len(dataset.rows)\n",
    "    correct = 0\n",
    "    for row in dataset.rows:\n",
    "       \n",
    "        correct += validate_row(node, row)                # validate example\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "def validate_row(node, row):                             \n",
    "    if (node.is_leaf == True):\n",
    "        projected = node.classification\n",
    "        actual = int(row[-1])\n",
    "        if (projected == actual):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    value = row[node.attribute_split_index]\n",
    "    if (value >= node.attribute_split_value):\n",
    "        return validate_row(node.left_child, row)\n",
    "    else:\n",
    "        return validate_row(node.right_child, row)\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "    f = open(\"wine-dataset.csv\")\n",
    "    original_file = f.read()\n",
    "    rowsplit_data = original_file.splitlines()\n",
    "\n",
    "\n",
    "    data = csvdata(\"\")\n",
    "    train_set = csvdata(\"\")\n",
    "    test_set = csvdata(\"\")\n",
    "    data.rows = [rows.split(',') for rows in rowsplit_data]\n",
    "\n",
    "    data.attributes = data.rows.pop(0)\n",
    "    print(data.attributes)\n",
    "\n",
    "                                                               \n",
    "                                                                    # true indicates numeric data. false in nominal data\n",
    "    data.attribute_types = ['true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'true', 'false']\n",
    "\n",
    "    classifier = data.attributes[-1]\n",
    "    data.classifier = classifier\n",
    "\n",
    "\n",
    "\n",
    "    for a in range(len(data.attributes)):                                # find index of classifier\n",
    "        if data.attributes[a] == data.classifier:\n",
    "            data.class_col_index = a\n",
    "        else:\n",
    "            data.class_col_index = range(len(data.attributes))[-1]\n",
    "\n",
    "    print (\"classifier is %d\" % data.class_col_index)\n",
    " \n",
    "    pre_processing(data)                                                  # preprocessing the dataset\n",
    "\n",
    "    train_set = copy.deepcopy(data)\n",
    "    test_set = copy.deepcopy(data)\n",
    "    validate_set = copy.deepcopy(data)\n",
    "    \n",
    "    train_set.rows = []\n",
    "    test_set.rows = []\n",
    "    validate_set.rows = []\n",
    "\n",
    "    K=10                                                                #K fold is done for 10 folds\n",
    "                                                                        # Stores accuracy of the 10 runs\n",
    "    accuracy = []\n",
    "\n",
    "    d=0\n",
    "    for k in range(K):\n",
    "        print (\"Doing fold \", k)\n",
    "        train_set.rows = [x for i, x in enumerate(data.rows) if i % K != k]\n",
    "        test_set.rows = [x for i, x in enumerate(data.rows) if i % K == k]\n",
    "\n",
    "        print (\"Number of training records: %d\" % len(train_set.rows))\n",
    "        print (\"Number of test records: %d\" % len(test_set.rows))\n",
    "        root = compute_decision_tree(train_set, None, classifier)\n",
    "\n",
    "                                                           # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set.rows:\n",
    "            result = classify(instance, root, test_set.class_col_index)\n",
    "            results.append(str(result) == str(instance[-1]))\n",
    "\n",
    "                                                             # Accuracy\n",
    "        acc = float(results.count(True))/float(len(results))\n",
    "        print (\"accuracy: %.4f\" % acc)\n",
    "        d+=acc\n",
    "\n",
    "    \n",
    "    print( \"Accuracy  %f \" % (d/10))                      #K-fold Accuracy\n",
    "    \n",
    "                                    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "Initially own version of Decision tree is implemented\n",
    "Then k fold cross validation was implemented using k=10\n",
    "k fold is done to select the final model\n",
    "Here k fold is implemented on one decision tree model\n",
    "Practically many other models are developed on the same problem statement and k fold accuracy is found for each model\n",
    "Then the model with best accuracy is implemented on practical scale."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Decision tree Q1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
